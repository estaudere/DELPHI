{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-7d7fc5b5-6b80-4243-9d38-cea60aef96d7",
    "tags": []
   },
   "source": [
    "# DELPHI COVID-19 Case Prediction Tool\n",
    "\n",
    "This is an interactive model version of the model displayed on [CovidAnalytics](https://www.covidanalytics.io/projections).\n",
    "\n",
    "Use the menu to Run All cells. To use this app, historical data per location must be given in the form `Cases_[Country]_[Province].csv`. If the province/state is not applicable, label it *None*.\n",
    "\n",
    "The model also requires a population dataset. Each location is a row in this CSV file (`Population_Global.csv`) with the headers *Continent, Country, Province, and pop2016*. This serves as the list of geographies the model will iterate over.\n",
    "\n",
    "For example, the model may have these files:\n",
    "* `Cases_US_California.csv` with the following headers: *country, province, date, day_since100, case_cnt, death_cnt*\n",
    "* `Cases_US_Florida.csv`\n",
    "* Similar case information for other states\n",
    "* A population CSV `Population_Global.csv`\n",
    "\n",
    "**How to upload files:** Use the `Upload` button to select a file, and the `Save File` button to save the file. Note: Files must be selected and saved one at a time.\n",
    "\n",
    "**How to run the model:** After the files have finished uploading, use the `Run Model` button to start the model. Note that this may take a few minutes for each geography (up to five hours if the world dataset is used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to switch on/off the raw code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toggle on/off the raw code\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to switch on/off the raw code\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the files using the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cell_id": "00001-5becd88e-17b2-41fd-9f99-8ef9263b8278",
    "execution_millis": 2983,
    "execution_start": 1602307604866,
    "output_cleared": false,
    "scrolled": true,
    "source_hash": "2057ec6e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ddc713954c449259fca4640e3d43fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59e40f3c5be46248d2e6c937d999e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(description='Save File', style=ButtonStyle()), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "myupload = widgets.FileUpload(\n",
    "    accept = '.csv', \n",
    "    multiple = False  \n",
    ")\n",
    "display(myupload)\n",
    "\n",
    "\n",
    "button = widgets.Button(description='Save File')\n",
    "out = widgets.Output()\n",
    "def on_button_clicked(_):\n",
    "      with out:\n",
    "          uploaded_filename = next(iter(myupload.value))\n",
    "          content = myupload.value[uploaded_filename]['content']\n",
    "          with open(f'./data/processed/{uploaded_filename}', 'wb') as f: f.write(content)\n",
    "          print(f'\\'{uploaded_filename}\\' successfully saved.')\n",
    "button.on_click(on_button_clicked)\n",
    "widgets.VBox([button,out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the button below to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "cell_id": "00000-710b1a33-40a2-4c34-9e58-979e03c748f3",
    "execution_millis": 1318725,
    "execution_start": 1602303037722,
    "is_code_hidden": true,
    "output_cleared": false,
    "source_hash": "fd97a723"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1c446bc50345199ad7fdae69692866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(description='Run Model', style=ButtonStyle()), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Authors: Hamza Tazi Bouardi (htazi@mit.edu), Michael L. Li (mlli@mit.edu), Omar Skali Lami (oskali@mit.edu)\n",
    "  # Authors: Hamza Tazi Bouardi (htazi@mit.edu), Michael L. Li (mlli@mit.edu), Omar Skali Lami (oskali@mit.edu)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.optimize import minimize\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from multiprocessing import set_start_method\n",
    "\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "from DELPHI_utils_V3 import (\n",
    "    DELPHIDataCreator, DELPHIAggregations, DELPHIDataSaver, get_initial_conditions, mape\n",
    ")\n",
    "from DELPHI_params_V3 import (\n",
    "    date_MATHEMATICA, default_parameter_list, default_bounds_params,\n",
    "    validcases_threshold, IncubeD, RecoverID, RecoverHD, DetectD,\n",
    "    VentilatedD, default_maxT, p_v, p_d, p_h, max_iter\n",
    ")\n",
    "import os\n",
    "# import yaml\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# with open(\"config.yml\", \"r\") as ymlfile:\n",
    "#     CONFIG = yaml.load(ymlfile, Loader=yaml.BaseLoader)\n",
    "# CONFIG_FILEPATHS = CONFIG[\"filepaths\"]\n",
    "# USER_RUNNING = \"neha\"\n",
    "\n",
    "button = widgets.Button(description='Run Model')\n",
    "out = widgets.Output()\n",
    "def on_button_clicked(_):\n",
    "      with out:\n",
    "            clear_output()\n",
    "            time_beginning = time.time()\n",
    "            yesterday = \"\".join(str(datetime.now().date() - timedelta(days=1)).split(\"-\"))\n",
    "            PATH_TO_FOLDER_DANGER_MAP = \"./data\"\n",
    "            # PATH_TO_WEBSITE_PREDICTED = CONFIG_FILEPATHS[\"website\"][USER_RUNNING]\n",
    "            popcountries = pd.read_csv(\n",
    "                PATH_TO_FOLDER_DANGER_MAP + f\"/processed/Population_Global.csv\"\n",
    "            )\n",
    "            popcountries[\"tuple_area\"] = list(zip(popcountries.Continent, popcountries.Country, popcountries.Province))\n",
    "\n",
    "            def solve_and_predict_area(\n",
    "                    tuple_area_: tuple, yesterday_: str, allowed_deviation_: float, pastparameters_: pd.DataFrame,\n",
    "            ):\n",
    "                time_entering = time.time()\n",
    "                continent, country, province = tuple_area_\n",
    "                # print(continent, country, province)\n",
    "                country_sub = country.replace(\" \", \"_\")\n",
    "                province_sub = province.replace(\" \", \"_\")\n",
    "                path = PATH_TO_FOLDER_DANGER_MAP +    f\"/processed/Cases_{country_sub}_{province_sub}.csv\"\n",
    "                # print(path)\n",
    "                if Path(path).is_file():\n",
    "                    totalcases = pd.read_csv(path)\n",
    "                    if totalcases.day_since100.max() < 0:\n",
    "                        print(f\"Not enough cases for Continent={continent}, Country={country} and Province={province}\")\n",
    "                        return None\n",
    "\n",
    "                    print(country + \", \" + province)\n",
    "                    if pastparameters_ is not None:\n",
    "                        parameter_list_total = pastparameters_[\n",
    "                            (pastparameters_.Country == country) &\n",
    "                            (pastparameters_.Province == province)\n",
    "                            ].reset_index(drop=True)\n",
    "                        if len(parameter_list_total) > 0:\n",
    "                            parameter_list_line = parameter_list_total.iloc[-1, :].values.tolist()\n",
    "                            parameter_list = parameter_list_line[5:]\n",
    "                            # Allowing a 5% drift for states with past predictions, starting in the 5th position are the parameters\n",
    "                            param_list_lower = [x - 0.1 * abs(x) for x in parameter_list]\n",
    "                            param_list_upper = [x + 0.1 * abs(x) for x in parameter_list]\n",
    "                            bounds_params = [(lower, upper)\n",
    "                                             for lower, upper in zip(param_list_lower, param_list_upper)]\n",
    "                            date_day_since100 = pd.to_datetime(parameter_list_line[3])\n",
    "                            validcases = totalcases[\n",
    "                                (totalcases.day_since100 >= 0) &\n",
    "                                (totalcases.date <= str((pd.to_datetime(yesterday_) + timedelta(days=1)).date()))\n",
    "                                ][[\"day_since100\", \"case_cnt\", \"death_cnt\"]].reset_index(drop=True)\n",
    "            #                parameter_list.insert(5, 0.2)\n",
    "            #                bounds_params.insert(5, (0, 0.5))\n",
    "            #                parameter_list.insert(8, 0.1)\n",
    "            #                bounds_params.insert(8, (0, 5))\n",
    "            #                parameter_list.insert(9,(len(validcases)-1) - 10)\n",
    "            #                bounds_params.insert(9,(0, len(validcases)-1))\n",
    "            #                parameter_list.insert(10, 1)\n",
    "            #                bounds_params.insert(10, (0.1, 5))\n",
    "                            bounds_params = tuple(bounds_params)\n",
    "                        else:\n",
    "                            # Otherwise use established lower/upper bounds\n",
    "                            parameter_list = default_parameter_list\n",
    "                            bounds_params = default_bounds_params\n",
    "                            date_day_since100 = pd.to_datetime(totalcases.loc[totalcases.day_since100 == 0, \"date\"].iloc[-1])\n",
    "                            validcases = totalcases[\n",
    "                                (totalcases.day_since100 >= 0) &\n",
    "                                (totalcases.date <= str((pd.to_datetime(yesterday_) + timedelta(days=1)).date()))\n",
    "                                ][[\"day_since100\", \"case_cnt\", \"death_cnt\"]].reset_index(drop=True)\n",
    "                    else:\n",
    "                        # Otherwise use established lower/upper bounds\n",
    "                        parameter_list = default_parameter_list\n",
    "                        bounds_params = default_bounds_params\n",
    "                        date_day_since100 = pd.to_datetime(totalcases.loc[totalcases.day_since100 == 0, \"date\"].iloc[-1])\n",
    "                        validcases = totalcases[\n",
    "                            (totalcases.day_since100 >= 0) &\n",
    "                            (totalcases.date <= str((pd.to_datetime(yesterday_) + timedelta(days=1)).date()))\n",
    "                            ][[\"day_since100\", \"case_cnt\", \"death_cnt\"]].reset_index(drop=True)\n",
    "                    # Now we start the modeling part:\n",
    "                    if len(validcases) > validcases_threshold:\n",
    "                        PopulationT = popcountries[\n",
    "                            (popcountries.Country == country) & (popcountries.Province == province)\n",
    "                            ].pop2016.iloc[-1]\n",
    "                        # We do not scale\n",
    "                        N = PopulationT\n",
    "                        PopulationI = validcases.loc[0, \"case_cnt\"]\n",
    "                        PopulationR = validcases.loc[0, \"death_cnt\"] * 5\n",
    "                        PopulationD = validcases.loc[0, \"death_cnt\"]\n",
    "                        PopulationCI = PopulationI - PopulationD - PopulationR\n",
    "                        \"\"\"\n",
    "                        Fixed Parameters based on meta-analysis:\n",
    "                        p_h: Hospitalization Percentage\n",
    "                        RecoverHD: Average Days till Recovery\n",
    "                        VentilationD: Number of Days on Ventilation for Ventilated Patients\n",
    "                        maxT: Maximum # of Days Modeled\n",
    "                        p_d: Percentage of True Cases Detected\n",
    "                        p_v: Percentage of Hospitalized Patients Ventilated,\n",
    "                        balance: Ratio of Fitting between cases and deaths\n",
    "                        \"\"\"\n",
    "                        # Currently fit on alpha, a and b, r_dth,\n",
    "                        # & initial condition of exposed state and infected state\n",
    "                        # Maximum timespan of prediction, defaulted to go to 15/06/2020\n",
    "                        maxT = (default_maxT - date_day_since100).days + 1\n",
    "                        \"\"\" Fit on Total Cases \"\"\"\n",
    "                        t_cases = validcases[\"day_since100\"].tolist() - validcases.loc[0, \"day_since100\"]\n",
    "                        validcases_nondeath = validcases[\"case_cnt\"].tolist()\n",
    "                        validcases_death = validcases[\"death_cnt\"].tolist()\n",
    "                        balance = validcases_nondeath[-1] / max(validcases_death[-1], 10) / 3\n",
    "                        fitcasesnd = validcases_nondeath\n",
    "                        fitcasesd = validcases_death\n",
    "                        GLOBAL_PARAMS_FIXED = (\n",
    "                            N, PopulationCI, PopulationR, PopulationD, PopulationI, p_d, p_h, p_v\n",
    "                        )\n",
    "\n",
    "                        def model_covid(\n",
    "                                t, x, alpha, days, r_s, r_dth, p_dth, r_dthdecay, k1, k2, jump, t_jump, std_normal\n",
    "                        ):\n",
    "                            \"\"\"\n",
    "                            SEIR + Undetected, Deaths, Hospitalized, corrected with ArcTan response curve\n",
    "                            alpha: Infection rate\n",
    "                            days: Median day of action\n",
    "                            r_s: Median rate of action\n",
    "                            p_dth: Mortality rate\n",
    "                            k1: Internal parameter 1\n",
    "                            k2: Internal parameter 2\n",
    "                            y = [0 S, 1 E,  2 I, 3 AR,   4 DHR,  5 DQR, 6 AD,\n",
    "                            7 DHD, 8 DQD, 9 R, 10 D, 11 TH, 12 DVR,13 DVD, 14 DD, 15 DT]\n",
    "                            \"\"\"\n",
    "                            r_i = np.log(2) / IncubeD  # Rate of infection leaving incubation phase\n",
    "                            r_d = np.log(2) / DetectD  # Rate of detection\n",
    "                            r_ri = np.log(2) / RecoverID  # Rate of recovery not under infection\n",
    "                            r_rh = np.log(2) / RecoverHD  # Rate of recovery under hospitalization\n",
    "                            r_rv = np.log(2) / VentilatedD  # Rate of recovery under ventilation\n",
    "                            gamma_t = (2 / np.pi) * np.arctan(-(t - days) / 20 * r_s) + 1 +  jump * np.exp(-(t - t_jump)**2 /(2 * std_normal ** 2))\n",
    "                            # gamma_t = (2 / np.pi) * np.arctan(-(t - days) / 20 * r_s) + 1 + jump * (np.arctan(t - t_jump) + np.pi / 2) * min(1, 2 / np.pi * np.arctan( - (t - t_jump)/ 20 * r_decay) + 1)\n",
    "\n",
    "                            # if t < t_jump:\n",
    "                            #     gamma_t = (2 / np.pi) * np.arctan(-(t - days) / 20 * r_s) + 1\n",
    "                            # else:\n",
    "                            #     gamma_t = (2 / np.pi) * np.arctan(-(t - days) / 20 * r_s) + 1 + jump\n",
    "                            p_dth_mod = (2 / np.pi) * (p_dth - 0.01) * (np.arctan(- t / 20 * r_dthdecay) + np.pi / 2) + 0.01\n",
    "                            assert len(x) == 16, f\"Too many input variables, got {len(x)}, expected 16\"\n",
    "                            S, E, I, AR, DHR, DQR, AD, DHD, DQD, R, D, TH, DVR, DVD, DD, DT = x\n",
    "                            # Equations on main variables\n",
    "                            dSdt = -alpha * gamma_t * S * I / N\n",
    "                            dEdt = alpha * gamma_t * S * I / N - r_i * E\n",
    "                            dIdt = r_i * E - r_d * I\n",
    "                            dARdt = r_d * (1 - p_dth_mod) * (1 - p_d) * I - r_ri * AR\n",
    "                            dDHRdt = r_d * (1 - p_dth_mod) * p_d * p_h * I - r_rh * DHR\n",
    "                            dDQRdt = r_d * (1 - p_dth_mod) * p_d * (1 - p_h) * I - r_ri * DQR\n",
    "                            dADdt = r_d * p_dth_mod * (1 - p_d) * I - r_dth * AD\n",
    "                            dDHDdt = r_d * p_dth_mod * p_d * p_h * I - r_dth * DHD\n",
    "                            dDQDdt = r_d * p_dth_mod * p_d * (1 - p_h) * I - r_dth * DQD\n",
    "                            dRdt = r_ri * (AR + DQR) + r_rh * DHR\n",
    "                            dDdt = r_dth * (AD + DQD + DHD)\n",
    "                            # Helper states (usually important for some kind of output)\n",
    "                            dTHdt = r_d * p_d * p_h * I\n",
    "                            dDVRdt = r_d * (1 - p_dth_mod) * p_d * p_h * p_v * I - r_rv * DVR\n",
    "                            dDVDdt = r_d * p_dth_mod * p_d * p_h * p_v * I - r_dth * DVD\n",
    "                            dDDdt = r_dth * (DHD + DQD)\n",
    "                            dDTdt = r_d * p_d * I\n",
    "                            return [\n",
    "                                dSdt, dEdt, dIdt, dARdt, dDHRdt, dDQRdt, dADdt, dDHDdt, dDQDdt,\n",
    "                                dRdt, dDdt, dTHdt, dDVRdt, dDVDdt, dDDdt, dDTdt\n",
    "                            ]\n",
    "\n",
    "                        def residuals_totalcases(params):\n",
    "                            \"\"\"\n",
    "                            Wanted to start with solve_ivp because figures will be faster to debug\n",
    "                            params: (alpha, days, r_s, r_dth, p_dth, k1, k2), fitted parameters of the model\n",
    "                            \"\"\"\n",
    "                            # Variables Initialization for the ODE system\n",
    "                            alpha, days, r_s, r_dth, p_dth, r_dthdecay, k1, k2, jump, t_jump, std_normal = params\n",
    "                            params = (\n",
    "                                max(alpha, 0), days, max(r_s, 0), max(r_dth, 0), max(min(p_dth, 1), 0), max(min(r_dthdecay, 1), 0),\n",
    "                                     max(k1, 0), max(k2, 0), max(jump, 0), max(t_jump, 0),max(std_normal, 0)\n",
    "                            )\n",
    "                            x_0_cases = get_initial_conditions(\n",
    "                                params_fitted=params,\n",
    "                                global_params_fixed=GLOBAL_PARAMS_FIXED\n",
    "                            )\n",
    "                            x_sol = solve_ivp(\n",
    "                                fun=model_covid,\n",
    "                                y0=x_0_cases,\n",
    "                                t_span=[t_cases[0], t_cases[-1]],\n",
    "                                t_eval=t_cases,\n",
    "                                args=tuple(params)\n",
    "                            ).y\n",
    "                            weights = list(range(1, len(fitcasesnd) + 1))\n",
    "                            # weights[-15:] =[x + 50 for x in weights[-15:]]\n",
    "                            residuals_value = sum(\n",
    "                                np.multiply((x_sol[15, :] - fitcasesnd) ** 2, weights)\n",
    "                                + balance * balance * np.multiply((x_sol[14, :] - fitcasesd) ** 2, weights)\n",
    "                            )\n",
    "                            return residuals_value\n",
    "\n",
    "                        # def last_point(params):\n",
    "                        #     alpha, days, r_s, r_dth, p_dth, k1, k2 = params\n",
    "                        #     params = max(alpha, 0), days, max(r_s, 0), max(r_dth, 0), max(min(p_dth, 1), 0), max(k1, 0), max(k2, 0)\n",
    "                        #     x_0_cases = get_initial_conditions(\n",
    "                        #         params_fitted=params,\n",
    "                        #         global_params_fixed=GLOBAL_PARAMS_FIXED\n",
    "                        #     )\n",
    "                        #     x_sol = solve_ivp(\n",
    "                        #         fun=model_covid,\n",
    "                        #         y0=x_0_cases,\n",
    "                        #         t_span=[t_cases[0], t_cases[-1]],\n",
    "                        #         t_eval=t_cases,\n",
    "                        #         args=tuple(params),\n",
    "                        #     ).y\n",
    "                        #     return x_sol[14:16,-1]\n",
    "                        # nlcons = NonlinearConstraint(last_point,\n",
    "                        #                              [fitcasesd[-1] * (1 - allowed_deviation_), fitcasesnd[-1] * (1 - allowed_deviation_) ],\n",
    "                        #                              [fitcasesd[-1] * (1 + allowed_deviation_), fitcasesnd[-1] * (1 + allowed_deviation_) ])\n",
    "                        output = minimize(\n",
    "                            residuals_totalcases,\n",
    "                            parameter_list,\n",
    "                            method='tnc',  # Can't use Nelder-Mead if I want to put bounds on the params\n",
    "                            bounds=bounds_params,\n",
    "                            options={'maxiter': max_iter}\n",
    "                        )\n",
    "                        best_params = output.x\n",
    "                        t_predictions = [i for i in range(maxT)]\n",
    "\n",
    "                        def solve_best_params_and_predict(optimal_params):\n",
    "                            # Variables Initialization for the ODE system\n",
    "                            x_0_cases = get_initial_conditions(\n",
    "                                params_fitted=optimal_params,\n",
    "                                global_params_fixed=GLOBAL_PARAMS_FIXED\n",
    "                            )\n",
    "                            x_sol_best = solve_ivp(\n",
    "                                fun=model_covid,\n",
    "                                y0=x_0_cases,\n",
    "                                t_span=[t_predictions[0], t_predictions[-1]],\n",
    "                                t_eval=t_predictions,\n",
    "                                args=tuple(optimal_params),\n",
    "                            ).y\n",
    "                            return x_sol_best\n",
    "\n",
    "                        x_sol_final = solve_best_params_and_predict(best_params)\n",
    "                        data_creator = DELPHIDataCreator(\n",
    "                            x_sol_final=x_sol_final, date_day_since100=date_day_since100, best_params=best_params,\n",
    "                            continent=continent, country=country, province=province, testing_data_included=False\n",
    "                        )\n",
    "                        # Creating the parameters dataset for this (Continent, Country, Province)\n",
    "                        mape_data = (\n",
    "                                            mape(fitcasesnd, x_sol_final[15, :len(fitcasesnd)]) +\n",
    "                                            mape(fitcasesd, x_sol_final[14, :len(fitcasesd)])\n",
    "                                    ) / 2\n",
    "                        if len(fitcasesnd) > 15:\n",
    "                            mape_data_2 = (\n",
    "                                                  mape(fitcasesnd[-15:], x_sol_final[15, len(fitcasesnd) - 15:len(fitcasesnd)]) +\n",
    "                                                  mape(fitcasesd[-15:], x_sol_final[14, len(fitcasesnd) - 15:len(fitcasesd)])\n",
    "                                          ) / 2\n",
    "                            print(f\"In-Sample MAPE Last 15 Days {country, province}: {round(mape_data_2, 3)} %\")\n",
    "                        df_parameters_cont_country_prov = data_creator.create_dataset_parameters(mape_data)\n",
    "                        # Creating the datasets for predictions of this (Continent, Country, Province)\n",
    "                        df_predictions_since_today_cont_country_prov, df_predictions_since_100_cont_country_prov = (\n",
    "                            data_creator.create_datasets_predictions()\n",
    "                        )\n",
    "                        print(\n",
    "                            f\"Finished predicting for Continent={continent}, Country={country} and Province={province} in \" +\n",
    "                            f\"{round(time.time() - time_entering, 2)} seconds\"\n",
    "                        )\n",
    "                        return (\n",
    "                            df_parameters_cont_country_prov, df_predictions_since_today_cont_country_prov,\n",
    "                            df_predictions_since_100_cont_country_prov, output\n",
    "                        )\n",
    "                    else:  # len(validcases) <= 7\n",
    "                        print(f\"Not enough historical data (less than a week)\" +\n",
    "                              f\"for Continent={continent}, Country={country} and Province={province}\")\n",
    "                        return None\n",
    "                else:  # file for that tuple (country, province) doesn't exist in processed files\n",
    "                    return None\n",
    "\n",
    "            def pool_proc(f, alist, n_cpu):\n",
    "              pool = mp.Pool(processes = n_cpu)\n",
    "              pool.imap(f, alist)\n",
    "\n",
    "              pool.close()\n",
    "              pool.join()\n",
    "\n",
    "\n",
    "            if __name__ == \"__main__\":\n",
    "                popcountries = pd.read_csv(\n",
    "                    PATH_TO_FOLDER_DANGER_MAP + f\"/processed/Population_Global.csv\"\n",
    "                )\n",
    "                try:\n",
    "                    pastparameters = pd.read_csv(\n",
    "                        PATH_TO_FOLDER_DANGER_MAP + f\"predicted/Parameters_Global_V2_{yesterday}.csv\"\n",
    "                    )\n",
    "                except:\n",
    "                    pastparameters = None\n",
    "                # Initalizing lists of the different dataframes that will be concatenated in the end\n",
    "                list_df_global_predictions_since_today = []\n",
    "                list_df_global_predictions_since_100_cases = []\n",
    "                list_df_global_parameters = []\n",
    "                obj_value = 0\n",
    "                allowed_deviation = 0.02\n",
    "                solve_and_predict_area_partial = partial(\n",
    "                    solve_and_predict_area, yesterday_=yesterday, pastparameters_=pastparameters,\n",
    "                    allowed_deviation_=allowed_deviation\n",
    "                )\n",
    "\n",
    "                n_cpu = 6\n",
    "\n",
    "                popcountries[\"tuple_area\"] = list(zip(popcountries.Continent, popcountries.Country, popcountries.Province))\n",
    "                list_tuples = popcountries.tuple_area.tolist()\n",
    "\n",
    "                # areas = pool_proc(solve_and_predict_area_partial, list_tuples, n_cpu=n_cpu) # uses mp Pool\n",
    "\n",
    "                areas = map(solve_and_predict_area_partial, list_tuples) # uses map function\n",
    "\n",
    "                # areas = []\n",
    "                # for tup in list_tuples:\n",
    "                #   areas.append(solve_and_predict_area_partial(tup)) # uses for loop\n",
    "\n",
    "                for result_area in areas:\n",
    "                    if result_area is not None:\n",
    "                        print(\"run\")\n",
    "                        (\n",
    "                          df_parameters_cont_country_prov, df_predictions_since_today_cont_country_prov,\n",
    "                          df_predictions_since_100_cont_country_prov, output\n",
    "                        ) = result_area\n",
    "                        obj_value = obj_value + output.fun\n",
    "                        # Then we add it to the list of df to be concatenated to update the tracking df\n",
    "                        list_df_global_parameters.append(df_parameters_cont_country_prov)\n",
    "                        list_df_global_predictions_since_today.append(df_predictions_since_today_cont_country_prov)\n",
    "                        list_df_global_predictions_since_100_cases.append(df_predictions_since_100_cont_country_prov)\n",
    "\n",
    "                    else:\n",
    "                        continue\n",
    "                print(\"Finished the Multiprocessing for all areas\")\n",
    "                # pool.close()\n",
    "                # pool.join()\n",
    "\n",
    "                # Appending parameters, aggregations per country, per continent, and for the world\n",
    "                # for predictions today & since 100\n",
    "                today_date_str = \"\".join(str(datetime.now().date()).split(\"-\"))\n",
    "                df_global_parameters = pd.concat(list_df_global_parameters).sort_values(\n",
    "                  [\"Country\", \"Province\"]\n",
    "                ).reset_index(drop=True)\n",
    "                df_global_predictions_since_today = pd.concat(list_df_global_predictions_since_today)\n",
    "                df_global_predictions_since_today = DELPHIAggregations.append_all_aggregations(\n",
    "                    df_global_predictions_since_today\n",
    "                )\n",
    "                df_global_predictions_since_100_cases = pd.concat(list_df_global_predictions_since_100_cases)\n",
    "                df_global_predictions_since_100_cases = DELPHIAggregations.append_all_aggregations(\n",
    "                    df_global_predictions_since_100_cases\n",
    "                )\n",
    "                delphi_data_saver = DELPHIDataSaver(\n",
    "                    path_to_folder_danger_map=PATH_TO_FOLDER_DANGER_MAP,\n",
    "                    df_global_parameters=df_global_parameters,\n",
    "                    df_global_predictions_since_today=df_global_predictions_since_today,\n",
    "                    df_global_predictions_since_100_cases=df_global_predictions_since_100_cases,\n",
    "                )\n",
    "                delphi_data_saver.save_all_datasets(save_since_100_cases=False, website=False)\n",
    "                print(f\"Exported all 3 datasets to website & danger_map repositories, \"+\n",
    "                      f\"total runtime was {round((time.time() - time_beginning)/60, 2)} minutes\")\n",
    "        \n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "widgets.VBox([button,out])"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "547c580f-4911-46d8-af36-0111482a0789",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
